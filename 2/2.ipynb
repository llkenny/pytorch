{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac96c418220b46c5912f047bca3318a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting data/cifar-10-python.tar.gz to data/\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                             train=True,  \n",
    "                                             transform=transforms.ToTensor(), \n",
    "                                             download=True)\n",
    "\n",
    "\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.permute(1, 2, 0).numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=3072, out_features=400, bias=True)\n",
       "  (fc2): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (fc3): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (fc4): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(3072, 100, 10)\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]. Step [1/782]. Loss: 0.036\n",
      "Epoch [1/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [1/5]. Step [601/782]. Loss: 0.035\n",
      "Epoch [2/5]. Step [1/782]. Loss: 0.033\n",
      "Epoch [2/5]. Step [301/782]. Loss: 0.032\n",
      "Epoch [2/5]. Step [601/782]. Loss: 0.031\n",
      "Epoch [3/5]. Step [1/782]. Loss: 0.031\n",
      "Epoch [3/5]. Step [301/782]. Loss: 0.030\n",
      "Epoch [3/5]. Step [601/782]. Loss: 0.030\n",
      "Epoch [4/5]. Step [1/782]. Loss: 0.032\n",
      "Epoch [4/5]. Step [301/782]. Loss: 0.029\n",
      "Epoch [4/5]. Step [601/782]. Loss: 0.029\n",
      "Epoch [5/5]. Step [1/782]. Loss: 0.034\n",
      "Epoch [5/5]. Step [301/782]. Loss: 0.028\n",
      "Epoch [5/5]. Step [601/782]. Loss: 0.028\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_items = 0.0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "         # Обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        # Делаем предсказание\n",
    "        outputs = net(inputs)\n",
    "        # Рассчитываем лосс-функцию\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Делаем шаг назад по лоссу\n",
    "        loss.backward()\n",
    "        # Делаем шаг нашего оптимайзера\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.3f}')\n",
    "            running_loss, running_items = 0.0, 0.0\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  deer deer truck deer frog horse deer frog bird car cat ship ship car frog plane\n"
     ]
    }
   ],
   "source": [
    "print('Predicted: ', ' '.join(classes[predicted[j]] for j in range(len(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deer' 'frog' 'car' 'deer' 'frog' 'horse' 'frog' 'dog' 'dog' 'car' 'car'\n",
      " 'car' 'plane' 'car' 'bird' 'ship']\n",
      "['deer' 'deer' 'truck' 'deer' 'frog' 'horse' 'deer' 'frog' 'bird' 'car'\n",
      " 'cat' 'ship' 'ship' 'car' 'frog' 'plane']\n",
      "Accuracy is 0.375\n"
     ]
    }
   ],
   "source": [
    "gt = np.array([classes[labels[j]] for j in range(len(labels))])\n",
    "pred = np.array([classes[predicted[j]] for j in range(len(labels))])\n",
    "\n",
    "print(gt)\n",
    "print(pred)\n",
    "print(f'Accuracy is {(gt == pred).sum() / len(gt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]. Step [1/782]. Loss: 0.036\n",
      "Epoch [1/10]. Step [301/782]. Loss: 0.036\n",
      "Epoch [1/10]. Step [601/782]. Loss: 0.036\n",
      "Epoch [2/10]. Step [1/782]. Loss: 0.036\n",
      "Epoch [2/10]. Step [301/782]. Loss: 0.036\n",
      "Epoch [2/10]. Step [601/782]. Loss: 0.036\n",
      "Epoch [3/10]. Step [1/782]. Loss: 0.036\n",
      "Epoch [3/10]. Step [301/782]. Loss: 0.036\n",
      "Epoch [3/10]. Step [601/782]. Loss: 0.034\n",
      "Epoch [4/10]. Step [1/782]. Loss: 0.034\n",
      "Epoch [4/10]. Step [301/782]. Loss: 0.032\n",
      "Epoch [4/10]. Step [601/782]. Loss: 0.032\n",
      "Epoch [5/10]. Step [1/782]. Loss: 0.030\n",
      "Epoch [5/10]. Step [301/782]. Loss: 0.031\n",
      "Epoch [5/10]. Step [601/782]. Loss: 0.030\n",
      "Epoch [6/10]. Step [1/782]. Loss: 0.032\n",
      "Epoch [6/10]. Step [301/782]. Loss: 0.030\n",
      "Epoch [6/10]. Step [601/782]. Loss: 0.030\n",
      "Epoch [7/10]. Step [1/782]. Loss: 0.031\n",
      "Epoch [7/10]. Step [301/782]. Loss: 0.029\n",
      "Epoch [7/10]. Step [601/782]. Loss: 0.029\n",
      "Epoch [8/10]. Step [1/782]. Loss: 0.027\n",
      "Epoch [8/10]. Step [301/782]. Loss: 0.028\n",
      "Epoch [8/10]. Step [601/782]. Loss: 0.028\n",
      "Epoch [9/10]. Step [1/782]. Loss: 0.028\n",
      "Epoch [9/10]. Step [301/782]. Loss: 0.027\n",
      "Epoch [9/10]. Step [601/782]. Loss: 0.027\n",
      "Epoch [10/10]. Step [1/782]. Loss: 0.031\n",
      "Epoch [10/10]. Step [301/782]. Loss: 0.026\n",
      "Epoch [10/10]. Step [601/782]. Loss: 0.026\n",
      "Accuracy is 0.3125\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 8 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(8 * hidden_dim, 8 * hidden_dim)\n",
    "        self.fc4 = nn.Linear(8 * hidden_dim, 8 * hidden_dim)\n",
    "        self.fc5 = nn.Linear(8 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc6 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc7 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc7(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(3072, 100, 10)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.0)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_items = 0.0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "         # Обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        # Делаем предсказание\n",
    "        outputs = net(inputs)\n",
    "        # Рассчитываем лосс-функцию\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Делаем шаг назад по лоссу\n",
    "        loss.backward()\n",
    "        # Делаем шаг нашего оптимайзера\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.3f}')\n",
    "            running_loss, running_items = 0.0, 0.0\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "gt = np.array([classes[labels[j]] for j in range(len(labels))])\n",
    "pred = np.array([classes[predicted[j]] for j in range(len(labels))])\n",
    "\n",
    "print(f'Accuracy is {(gt == pred).sum() / len(gt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: Accuracy is 0.3125 при незначительном увеличении слоёв, нейронов и эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]. Step [1/782]. Loss: 0.036\n",
      "Epoch [1/100]. Step [301/782]. Loss: 0.035\n",
      "Epoch [1/100]. Step [601/782]. Loss: 0.032\n",
      "Epoch [2/100]. Step [1/782]. Loss: 0.032\n",
      "Epoch [2/100]. Step [301/782]. Loss: 0.030\n",
      "Epoch [2/100]. Step [601/782]. Loss: 0.030\n",
      "Epoch [3/100]. Step [1/782]. Loss: 0.029\n",
      "Epoch [3/100]. Step [301/782]. Loss: 0.029\n",
      "Epoch [3/100]. Step [601/782]. Loss: 0.028\n",
      "Epoch [4/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [4/100]. Step [301/782]. Loss: 0.027\n",
      "Epoch [4/100]. Step [601/782]. Loss: 0.027\n",
      "Epoch [5/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [5/100]. Step [301/782]. Loss: 0.026\n",
      "Epoch [5/100]. Step [601/782]. Loss: 0.026\n",
      "Epoch [6/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [6/100]. Step [301/782]. Loss: 0.025\n",
      "Epoch [6/100]. Step [601/782]. Loss: 0.025\n",
      "Epoch [7/100]. Step [1/782]. Loss: 0.028\n",
      "Epoch [7/100]. Step [301/782]. Loss: 0.025\n",
      "Epoch [7/100]. Step [601/782]. Loss: 0.025\n",
      "Epoch [8/100]. Step [1/782]. Loss: 0.022\n",
      "Epoch [8/100]. Step [301/782]. Loss: 0.024\n",
      "Epoch [8/100]. Step [601/782]. Loss: 0.024\n",
      "Epoch [9/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [9/100]. Step [301/782]. Loss: 0.024\n",
      "Epoch [9/100]. Step [601/782]. Loss: 0.024\n",
      "Epoch [10/100]. Step [1/782]. Loss: 0.030\n",
      "Epoch [10/100]. Step [301/782]. Loss: 0.023\n",
      "Epoch [10/100]. Step [601/782]. Loss: 0.023\n",
      "Epoch [11/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [11/100]. Step [301/782]. Loss: 0.023\n",
      "Epoch [11/100]. Step [601/782]. Loss: 0.023\n",
      "Epoch [12/100]. Step [1/782]. Loss: 0.025\n",
      "Epoch [12/100]. Step [301/782]. Loss: 0.022\n",
      "Epoch [12/100]. Step [601/782]. Loss: 0.022\n",
      "Epoch [13/100]. Step [1/782]. Loss: 0.027\n",
      "Epoch [13/100]. Step [301/782]. Loss: 0.022\n",
      "Epoch [13/100]. Step [601/782]. Loss: 0.022\n",
      "Epoch [14/100]. Step [1/782]. Loss: 0.023\n",
      "Epoch [14/100]. Step [301/782]. Loss: 0.022\n",
      "Epoch [14/100]. Step [601/782]. Loss: 0.022\n",
      "Epoch [15/100]. Step [1/782]. Loss: 0.021\n",
      "Epoch [15/100]. Step [301/782]. Loss: 0.021\n",
      "Epoch [15/100]. Step [601/782]. Loss: 0.021\n",
      "Epoch [16/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [16/100]. Step [301/782]. Loss: 0.021\n",
      "Epoch [16/100]. Step [601/782]. Loss: 0.021\n",
      "Epoch [17/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [17/100]. Step [301/782]. Loss: 0.021\n",
      "Epoch [17/100]. Step [601/782]. Loss: 0.021\n",
      "Epoch [18/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [18/100]. Step [301/782]. Loss: 0.020\n",
      "Epoch [18/100]. Step [601/782]. Loss: 0.020\n",
      "Epoch [19/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [19/100]. Step [301/782]. Loss: 0.020\n",
      "Epoch [19/100]. Step [601/782]. Loss: 0.020\n",
      "Epoch [20/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [20/100]. Step [301/782]. Loss: 0.020\n",
      "Epoch [20/100]. Step [601/782]. Loss: 0.020\n",
      "Epoch [21/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [21/100]. Step [301/782]. Loss: 0.020\n",
      "Epoch [21/100]. Step [601/782]. Loss: 0.020\n",
      "Epoch [22/100]. Step [1/782]. Loss: 0.023\n",
      "Epoch [22/100]. Step [301/782]. Loss: 0.019\n",
      "Epoch [22/100]. Step [601/782]. Loss: 0.019\n",
      "Epoch [23/100]. Step [1/782]. Loss: 0.021\n",
      "Epoch [23/100]. Step [301/782]. Loss: 0.019\n",
      "Epoch [23/100]. Step [601/782]. Loss: 0.019\n",
      "Epoch [24/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [24/100]. Step [301/782]. Loss: 0.019\n",
      "Epoch [24/100]. Step [601/782]. Loss: 0.019\n",
      "Epoch [25/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [25/100]. Step [301/782]. Loss: 0.019\n",
      "Epoch [25/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [26/100]. Step [1/782]. Loss: 0.025\n",
      "Epoch [26/100]. Step [301/782]. Loss: 0.018\n",
      "Epoch [26/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [27/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [27/100]. Step [301/782]. Loss: 0.018\n",
      "Epoch [27/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [28/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [28/100]. Step [301/782]. Loss: 0.018\n",
      "Epoch [28/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [29/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [29/100]. Step [301/782]. Loss: 0.018\n",
      "Epoch [29/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [30/100]. Step [1/782]. Loss: 0.021\n",
      "Epoch [30/100]. Step [301/782]. Loss: 0.017\n",
      "Epoch [30/100]. Step [601/782]. Loss: 0.018\n",
      "Epoch [31/100]. Step [1/782]. Loss: 0.017\n",
      "Epoch [31/100]. Step [301/782]. Loss: 0.017\n",
      "Epoch [31/100]. Step [601/782]. Loss: 0.017\n",
      "Epoch [32/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [32/100]. Step [301/782]. Loss: 0.017\n",
      "Epoch [32/100]. Step [601/782]. Loss: 0.017\n",
      "Epoch [33/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [33/100]. Step [301/782]. Loss: 0.017\n",
      "Epoch [33/100]. Step [601/782]. Loss: 0.017\n",
      "Epoch [34/100]. Step [1/782]. Loss: 0.024\n",
      "Epoch [34/100]. Step [301/782]. Loss: 0.016\n",
      "Epoch [34/100]. Step [601/782]. Loss: 0.016\n",
      "Epoch [35/100]. Step [1/782]. Loss: 0.015\n",
      "Epoch [35/100]. Step [301/782]. Loss: 0.016\n",
      "Epoch [35/100]. Step [601/782]. Loss: 0.016\n",
      "Epoch [36/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [36/100]. Step [301/782]. Loss: 0.016\n",
      "Epoch [36/100]. Step [601/782]. Loss: 0.016\n",
      "Epoch [37/100]. Step [1/782]. Loss: 0.022\n",
      "Epoch [37/100]. Step [301/782]. Loss: 0.016\n",
      "Epoch [37/100]. Step [601/782]. Loss: 0.016\n",
      "Epoch [38/100]. Step [1/782]. Loss: 0.021\n",
      "Epoch [38/100]. Step [301/782]. Loss: 0.016\n",
      "Epoch [38/100]. Step [601/782]. Loss: 0.016\n",
      "Epoch [39/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [39/100]. Step [301/782]. Loss: 0.015\n",
      "Epoch [39/100]. Step [601/782]. Loss: 0.015\n",
      "Epoch [40/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [40/100]. Step [301/782]. Loss: 0.015\n",
      "Epoch [40/100]. Step [601/782]. Loss: 0.015\n",
      "Epoch [41/100]. Step [1/782]. Loss: 0.018\n",
      "Epoch [41/100]. Step [301/782]. Loss: 0.015\n",
      "Epoch [41/100]. Step [601/782]. Loss: 0.015\n",
      "Epoch [42/100]. Step [1/782]. Loss: 0.026\n",
      "Epoch [42/100]. Step [301/782]. Loss: 0.015\n",
      "Epoch [42/100]. Step [601/782]. Loss: 0.015\n",
      "Epoch [43/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [43/100]. Step [301/782]. Loss: 0.014\n",
      "Epoch [43/100]. Step [601/782]. Loss: 0.015\n",
      "Epoch [44/100]. Step [1/782]. Loss: 0.017\n",
      "Epoch [44/100]. Step [301/782]. Loss: 0.014\n",
      "Epoch [44/100]. Step [601/782]. Loss: 0.014\n",
      "Epoch [45/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [45/100]. Step [301/782]. Loss: 0.014\n",
      "Epoch [45/100]. Step [601/782]. Loss: 0.014\n",
      "Epoch [46/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [46/100]. Step [301/782]. Loss: 0.014\n",
      "Epoch [46/100]. Step [601/782]. Loss: 0.014\n",
      "Epoch [47/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [47/100]. Step [301/782]. Loss: 0.014\n",
      "Epoch [47/100]. Step [601/782]. Loss: 0.014\n",
      "Epoch [48/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [48/100]. Step [301/782]. Loss: 0.013\n",
      "Epoch [48/100]. Step [601/782]. Loss: 0.013\n",
      "Epoch [49/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [49/100]. Step [301/782]. Loss: 0.013\n",
      "Epoch [49/100]. Step [601/782]. Loss: 0.013\n",
      "Epoch [50/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [50/100]. Step [301/782]. Loss: 0.013\n",
      "Epoch [50/100]. Step [601/782]. Loss: 0.013\n",
      "Epoch [51/100]. Step [1/782]. Loss: 0.015\n",
      "Epoch [51/100]. Step [301/782]. Loss: 0.012\n",
      "Epoch [51/100]. Step [601/782]. Loss: 0.013\n",
      "Epoch [52/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [52/100]. Step [301/782]. Loss: 0.012\n",
      "Epoch [52/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [53/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [53/100]. Step [301/782]. Loss: 0.012\n",
      "Epoch [53/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [54/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [54/100]. Step [301/782]. Loss: 0.012\n",
      "Epoch [54/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [55/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [55/100]. Step [301/782]. Loss: 0.011\n",
      "Epoch [55/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [56/100]. Step [1/782]. Loss: 0.023\n",
      "Epoch [56/100]. Step [301/782]. Loss: 0.011\n",
      "Epoch [56/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [57/100]. Step [1/782]. Loss: 0.018\n",
      "Epoch [57/100]. Step [301/782]. Loss: 0.011\n",
      "Epoch [57/100]. Step [601/782]. Loss: 0.012\n",
      "Epoch [58/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [58/100]. Step [301/782]. Loss: 0.011\n",
      "Epoch [58/100]. Step [601/782]. Loss: 0.011\n",
      "Epoch [59/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [59/100]. Step [301/782]. Loss: 0.011\n",
      "Epoch [59/100]. Step [601/782]. Loss: 0.011\n",
      "Epoch [60/100]. Step [1/782]. Loss: 0.015\n",
      "Epoch [60/100]. Step [301/782]. Loss: 0.010\n",
      "Epoch [60/100]. Step [601/782]. Loss: 0.011\n",
      "Epoch [61/100]. Step [1/782]. Loss: 0.019\n",
      "Epoch [61/100]. Step [301/782]. Loss: 0.010\n",
      "Epoch [61/100]. Step [601/782]. Loss: 0.011\n",
      "Epoch [62/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [62/100]. Step [301/782]. Loss: 0.010\n",
      "Epoch [62/100]. Step [601/782]. Loss: 0.010\n",
      "Epoch [63/100]. Step [1/782]. Loss: 0.011\n",
      "Epoch [63/100]. Step [301/782]. Loss: 0.010\n",
      "Epoch [63/100]. Step [601/782]. Loss: 0.010\n",
      "Epoch [64/100]. Step [1/782]. Loss: 0.012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100]. Step [301/782]. Loss: 0.010\n",
      "Epoch [64/100]. Step [601/782]. Loss: 0.010\n",
      "Epoch [65/100]. Step [1/782]. Loss: 0.020\n",
      "Epoch [65/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [65/100]. Step [601/782]. Loss: 0.010\n",
      "Epoch [66/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [66/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [66/100]. Step [601/782]. Loss: 0.010\n",
      "Epoch [67/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [67/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [67/100]. Step [601/782]. Loss: 0.009\n",
      "Epoch [68/100]. Step [1/782]. Loss: 0.016\n",
      "Epoch [68/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [68/100]. Step [601/782]. Loss: 0.009\n",
      "Epoch [69/100]. Step [1/782]. Loss: 0.009\n",
      "Epoch [69/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [69/100]. Step [601/782]. Loss: 0.009\n",
      "Epoch [70/100]. Step [1/782]. Loss: 0.023\n",
      "Epoch [70/100]. Step [301/782]. Loss: 0.009\n",
      "Epoch [70/100]. Step [601/782]. Loss: 0.009\n",
      "Epoch [71/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [71/100]. Step [301/782]. Loss: 0.008\n",
      "Epoch [71/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [72/100]. Step [1/782]. Loss: 0.021\n",
      "Epoch [72/100]. Step [301/782]. Loss: 0.008\n",
      "Epoch [72/100]. Step [601/782]. Loss: 0.009\n",
      "Epoch [73/100]. Step [1/782]. Loss: 0.010\n",
      "Epoch [73/100]. Step [301/782]. Loss: 0.008\n",
      "Epoch [73/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [74/100]. Step [1/782]. Loss: 0.010\n",
      "Epoch [74/100]. Step [301/782]. Loss: 0.008\n",
      "Epoch [74/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [75/100]. Step [1/782]. Loss: 0.018\n",
      "Epoch [75/100]. Step [301/782]. Loss: 0.008\n",
      "Epoch [75/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [76/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [76/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [76/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [77/100]. Step [1/782]. Loss: 0.007\n",
      "Epoch [77/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [77/100]. Step [601/782]. Loss: 0.008\n",
      "Epoch [78/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [78/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [78/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [79/100]. Step [1/782]. Loss: 0.009\n",
      "Epoch [79/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [79/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [80/100]. Step [1/782]. Loss: 0.006\n",
      "Epoch [80/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [80/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [81/100]. Step [1/782]. Loss: 0.033\n",
      "Epoch [81/100]. Step [301/782]. Loss: 0.007\n",
      "Epoch [81/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [82/100]. Step [1/782]. Loss: 0.008\n",
      "Epoch [82/100]. Step [301/782]. Loss: 0.006\n",
      "Epoch [82/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [83/100]. Step [1/782]. Loss: 0.011\n",
      "Epoch [83/100]. Step [301/782]. Loss: 0.006\n",
      "Epoch [83/100]. Step [601/782]. Loss: 0.007\n",
      "Epoch [84/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [84/100]. Step [301/782]. Loss: 0.006\n",
      "Epoch [84/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [85/100]. Step [1/782]. Loss: 0.011\n",
      "Epoch [85/100]. Step [301/782]. Loss: 0.006\n",
      "Epoch [85/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [86/100]. Step [1/782]. Loss: 0.013\n",
      "Epoch [86/100]. Step [301/782]. Loss: 0.006\n",
      "Epoch [86/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [87/100]. Step [1/782]. Loss: 0.015\n",
      "Epoch [87/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [87/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [88/100]. Step [1/782]. Loss: 0.008\n",
      "Epoch [88/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [88/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [89/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [89/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [89/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [90/100]. Step [1/782]. Loss: 0.011\n",
      "Epoch [90/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [90/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [91/100]. Step [1/782]. Loss: 0.011\n",
      "Epoch [91/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [91/100]. Step [601/782]. Loss: 0.006\n",
      "Epoch [92/100]. Step [1/782]. Loss: 0.004\n",
      "Epoch [92/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [92/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [93/100]. Step [1/782]. Loss: 0.022\n",
      "Epoch [93/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [93/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [94/100]. Step [1/782]. Loss: 0.008\n",
      "Epoch [94/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [94/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [95/100]. Step [1/782]. Loss: 0.009\n",
      "Epoch [95/100]. Step [301/782]. Loss: 0.004\n",
      "Epoch [95/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [96/100]. Step [1/782]. Loss: 0.007\n",
      "Epoch [96/100]. Step [301/782]. Loss: 0.004\n",
      "Epoch [96/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [97/100]. Step [1/782]. Loss: 0.012\n",
      "Epoch [97/100]. Step [301/782]. Loss: 0.005\n",
      "Epoch [97/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [98/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [98/100]. Step [301/782]. Loss: 0.004\n",
      "Epoch [98/100]. Step [601/782]. Loss: 0.005\n",
      "Epoch [99/100]. Step [1/782]. Loss: 0.014\n",
      "Epoch [99/100]. Step [301/782]. Loss: 0.004\n",
      "Epoch [99/100]. Step [601/782]. Loss: 0.004\n",
      "Epoch [100/100]. Step [1/782]. Loss: 0.034\n",
      "Epoch [100/100]. Step [301/782]. Loss: 0.004\n",
      "Epoch [100/100]. Step [601/782]. Loss: 0.004\n",
      "Accuracy is 0.9375\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(3072, 100, 10)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.0)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_items = 0.0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "         # Обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        # Делаем предсказание\n",
    "        outputs = net(inputs)\n",
    "        # Рассчитываем лосс-функцию\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Делаем шаг назад по лоссу\n",
    "        loss.backward()\n",
    "        # Делаем шаг нашего оптимайзера\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.3f}')\n",
    "            running_loss, running_items = 0.0, 0.0\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "gt = np.array([classes[labels[j]] for j in range(len(labels))])\n",
    "pred = np.array([classes[predicted[j]] for j in range(len(labels))])\n",
    "\n",
    "print(f'Accuracy is {(gt == pred).sum() / len(gt)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: Accuracy is 0.9375 при значительном увеличении количества эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]. Step [1/782]. Loss: 0.037\n",
      "Epoch [1/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [1/5]. Step [601/782]. Loss: 0.036\n",
      "Epoch [2/5]. Step [1/782]. Loss: 0.036\n",
      "Epoch [2/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [2/5]. Step [601/782]. Loss: 0.036\n",
      "Epoch [3/5]. Step [1/782]. Loss: 0.036\n",
      "Epoch [3/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [3/5]. Step [601/782]. Loss: 0.036\n",
      "Epoch [4/5]. Step [1/782]. Loss: 0.036\n",
      "Epoch [4/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [4/5]. Step [601/782]. Loss: 0.036\n",
      "Epoch [5/5]. Step [1/782]. Loss: 0.036\n",
      "Epoch [5/5]. Step [301/782]. Loss: 0.036\n",
      "Epoch [5/5]. Step [601/782]. Loss: 0.036\n",
      "Accuracy is 0.0\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * hidden_dim)\n",
    "        self.fc2 = nn.Linear(4 * hidden_dim, 2 * hidden_dim)\n",
    "        self.fc3 = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(3072, 100, 10)\n",
    "net.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.0)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_items = 0.0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0], data[1]\n",
    "\n",
    "         # Обнуляем градиент\n",
    "        optimizer.zero_grad()\n",
    "        # Делаем предсказание\n",
    "        outputs = net(inputs)\n",
    "        # Рассчитываем лосс-функцию\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Делаем шаг назад по лоссу\n",
    "        loss.backward()\n",
    "        # Делаем шаг нашего оптимайзера\n",
    "        optimizer.step()\n",
    "\n",
    "        # выводим статистику о процессе обучения\n",
    "        running_loss += loss.item()\n",
    "        running_items += len(labels)\n",
    "        if i % 300 == 0:    # печатаем каждые 300 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}]. ' \\\n",
    "                  f'Step [{i + 1}/{len(train_loader)}]. ' \\\n",
    "                  f'Loss: {running_loss / running_items:.3f}')\n",
    "            running_loss, running_items = 0.0, 0.0\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "gt = np.array([classes[labels[j]] for j in range(len(labels))])\n",
    "pred = np.array([classes[predicted[j]] for j in range(len(labels))])\n",
    "\n",
    "print(f'Accuracy is {(gt == pred).sum() / len(gt)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат: сигмоида не подошла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: Наибольшее повышение Accuracy дало значительное увеличение эпох. При num_epochs = 100, Accuracy = 0.9375."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
